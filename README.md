# transformer-pytorch

This project is a minimal, educational implementation of the Transformer architecture â€” the foundation of modern large language models like GPT and BERT. Inspired by the original "Attention is All You Need" paper, this repo walks through each component of the model in clean, readable PyTorch code.

# What's Inside
Positional Encoding (including Rotary Positional Encoding)

Multi-Head Self-Attention

Layer Normalization

Feedforward Networks

Masking and Causal Attention

Full forward pass for language modeling
